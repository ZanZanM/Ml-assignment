# -*- coding: utf-8 -*-
"""Copy of assingment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KMP3lEp2BLQn8jiJ8pFfEbbmAmtvEgB7

# Machine Learning Assignment 2

## 1. Import Libraries and Data
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings("ignore")

# %matplotlib inline
import json
import string
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from prettytable import PrettyTable

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize

file_path = "/content/drive/MyDrive/zane/reviews.json"

nltk.download('punkt')
nltk.download('stopwords')
STOPWORD = set(stopwords.words('spanish'))

dataframe = pd.read_json(file_path)  ## reading json file using pandas attribute
dataframe.head()  ## getting top 5 rows of the dataframe

## creating empty list to get all reviews and the scores
reviews = list(); evaluations = list()

## iterating the reviews to get the text and corresponding scores/evaluations
for i in range(dataframe.shape[0]):
    review_file = dataframe["paper"].values[i]["review"]

    ## getting the total text present in each review file
    count = len(review_file)
    
    ## iterating over all text in a single review to get the review text and corressponding scores
    for j in range(count):
        reviews.append(review_file[j]["text"])
        evaluations.append(review_file[j]["evaluation"])

## creating another dataframe
df = pd.DataFrame()  ## creating empty dataframe
df["reviews"] = reviews  ## mapping all the reviews to the dataframe
df["evaluation"] = evaluations ## mapping all the evaluations to the dataframe
df["evaluation"] = df["evaluation"].apply(lambda x:int(x))  ## converting all evaluations from string to integer

print("Shape of the dataset is ",df.shape) ## getting shape
df.head()  ## getting top 5 values

"""* We have now all the reviews and corresponding evaluations in the daraframe.

### 1.1 Count of evaluations present in the dataset
"""

plot = df["evaluation"].value_counts().sort_index().plot(kind = "bar", xlabel = "evaluations", ylabel="count", title="evaluation distributions")

"""## 2. Text Preprocessing

### 2.1 checking for length of letters present in reviews
"""

## as we can see above there are some rows where not text is present. 
## We must check for the length of letters present in each line.
df["length_of_text"] = df["reviews"].apply(lambda x:len(x))

## checking for rows/text where no letter present.
df[df["length_of_text"] == 0]

## selecting the sentence where letters are more than 0
df1 = df[df["length_of_text"] > 0]

"""### 2.2 lowecase the entire text"""

## lowercasing the text using lambda anonymous function
df1["preprocessed_reviews"] = df1["reviews"].apply(lambda x:x.lower())

"""### 2.3 remove the special characters"""

preprocessed_text = list()
## iterating over loop to remove punctuations
for text in df1["preprocessed_reviews"].values:
    text = "".join(letter for letter in text if letter not in string.punctuation)
    preprocessed_text.append(text)

df1["preprocessed_reviews"] = preprocessed_text

## removing of spaces from both left and right side
df1["preprocessed_reviews"] = df1["preprocessed_reviews"].apply(lambda x :x.lstrip())
df1["preprocessed_reviews"] = df1["preprocessed_reviews"].apply(lambda x :x.strip())

"""### 2.4 removal of stopwords"""

## removal of stopwords
df1["preprocessed_reviews"] = df1['preprocessed_reviews'].apply(lambda x: " ".join(i for i in word_tokenize(x) if i not in STOPWORD))

"""### 2.5 removal of digits"""

## removing the digits
df1['preprocessed_reviews'] = df1['preprocessed_reviews'].apply(lambda x: " ".join(i for i in word_tokenize(x) if i.isdigit()!=True))

"""## 3. Task 2

Give the most important 10 featues (i.e. words) in positive and negative reviews.
"""

def frequent_words(data, num_of_words):
    """
    This function provides the frequent words present in the text corpus.
    """
    #creating a list of words present in the all text corpus
    corpus = [j for line in data.values for j in line.split()]

    ## using Counter method to create word_count of each unique word
    word_count = Counter(corpus)

    ## getting top n most common words
    n_words = word_count.most_common(num_of_words)

    return n_words

def plot_for_word_n_count(words, counts, size, label):

    ## bar plot using matplotlib
    fig = plt.figure(figsize=size)
    plot = plt.barh(words, counts, 0.75, color = "black")

    ## getting number count for each bar
    for i, v in enumerate(counts):
        plt.text(v + 1, i - 0.20, str(v), color='black', fontweight='bold')

    ## title, labels
    plt.title("word analysis", fontsize=15)
    plt.xlabel(label, fontsize=15)
    plt.ylabel("count", fontsize=15)
    plt.show()

"""### 3.1 Positive Words"""

## for positive features I have selected only those reviews where evaluation is greater than 0
df_positive = df1[df1["evaluation"] > 0]

## getting top 10 positive words
postive_words = frequent_words(df_positive["preprocessed_reviews"], 10)

## plot
pos_data = pd.DataFrame(postive_words)
plot_for_word_n_count(list(pos_data.iloc[:,0].values), list(pos_data.iloc[:,1].values), (15,5), "positive words")

"""### 3.2 Negative Words"""

## for negative features I have selected only those reviews where evaluation is less than 0
df_negative = df1[df1["evaluation"] < 0]

## getting top 10 positive words
negative_words = frequent_words(df_negative["preprocessed_reviews"], 10)

## plot
pos_data = pd.DataFrame(negative_words)
plot_for_word_n_count(list(pos_data.iloc[:,0].values), list(pos_data.iloc[:,1].values), (15,5), "negative words")

"""## 4. splitting of data"""

from sklearn.model_selection import train_test_split

y = df1["evaluation"].values   ## outputs
X = df1["preprocessed_reviews"].values   ## input text

## splitting the data keep 10% for testing and 90% for training
xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.15, stratify = y, random_state=42)  

## getting shape
xtrain.shape, ytrain.shape, xtest.shape, ytest.shape

"""## 5. Encoding the data
* Here I am going to use bagofwords method using countvectorizer from sklearn library.
"""

from sklearn.feature_extraction.text import CountVectorizer

enc = CountVectorizer()  ## creating an object of count vectorizer
reg = enc.fit(xtrain)  ## fitting the countvectorizer object with train data

## transforming the data
xtrain_bow = enc.transform(xtrain)
xtest_bow = enc.transform(xtest)

## getting shape
xtrain_bow.shape, xtest_bow.shape

"""## 6. Task 1
* You will build a linear regression model preferably using Scikit-learn library. 
* You will use unigram bag-of-words features as binary features. Make up few test reviews which do not exist in the training set.
"""

from sklearn.linear_model import LinearRegression

lr_reg = LinearRegression()  ## creating a linear regression object
reg = lr_reg.fit(xtrain_bow, ytrain)  ## fitting the linear regression model with xtrain and ytrain

## predicting on unseen data
lr_reg.predict(xtest_bow)